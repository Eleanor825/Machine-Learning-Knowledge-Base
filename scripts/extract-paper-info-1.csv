web-scraper-order,web-scraper-start-url,paper-title,paper-authors,paper-abstract,paper-year
"1665689176-3","https://arxiv.org/abs/1703.00573","Title:Generalization and Equilibrium in Generative Adversarial Nets (GANs)","Authors:Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, Yi Zhang","Abstract:  We show that training of generative adversarial network (GAN) may not have
good generalization properties; e.g., training may appear successful but the
trained distribution may be far from target distribution in standard metrics.
However, generalization does occur for a weaker metric called neural net
distance. It is also shown that an approximate pure equilibrium exists in the
discriminator/generator game for a special class of generators with natural
training objectives when generator capacity and training set sizes are
moderate.

This existence of equilibrium inspires MIX+GAN protocol, which can be
combined with any existing GAN training, and empirically shown to improve some
of them.","[Submitted on 2 Mar 2017 (v1), last revised 1 Aug 2017 (this version, v5)]"
"1665689178-4","https://arxiv.org/abs/1411.1784","Title:Conditional Generative Adversarial Nets","Authors:Mehdi Mirza, Simon Osindero","Abstract:  Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.","[Submitted on 6 Nov 2014]"
"1665689180-5","https://arxiv.org/abs/1705.09966","Title:Attribute-Guided Face Generation Using Conditional CycleGAN","Authors:Yongyi Lu, Yu-Wing Tai, Chi-Keung Tang","Abstract:  We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.","[Submitted on 28 May 2017 (v1), last revised 14 Nov 2018 (this version, v2)]"
"1665689182-6","https://arxiv.org/abs/1702.08431","Title:Boundary-Seeking Generative Adversarial Networks","Authors:R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, Yoshua Bengio","Abstract:  Generative adversarial networks (GANs) are a learning framework that rely on
training a discriminator to estimate a measure of difference between a target
and generated distributions. GANs, as normally formulated, rely on the
generated samples being completely differentiable w.r.t. the generative
parameters, and thus do not work for discrete data. We introduce a method for
training GANs with discrete data that uses the estimated difference measure
from the discriminator to compute importance weights for generated samples,
thus providing a policy gradient for training the generator. The importance
weights have a strong connection to the decision boundary of the discriminator,
and we call our method boundary-seeking GANs (BGANs). We demonstrate the
effectiveness of the proposed algorithm with discrete image and character-based
natural language generation. In addition, the boundary-seeking objective
extends to continuous data, which can be used to improve stability of training,
and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN)
bedrooms, and Imagenet without conditioning.","[Submitted on 27 Feb 2017 (v1), last revised 21 Feb 2018 (this version, v4)]"
"1665689185-7","https://arxiv.org/abs/1705.09558","Title:Bayesian GAN","Authors:Yunus Saatchi, Andrew Gordon Wilson","Abstract:  Generative adversarial networks (GANs) can implicitly learn rich
distributions over images, audio, and data which are hard to model with an
explicit likelihood. We present a practical Bayesian formulation for
unsupervised and semi-supervised learning with GANs. Within this framework, we
use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of
the generator and discriminator networks. The resulting approach is
straightforward and obtains good performance without any standard interventions
such as feature matching, or mini-batch discrimination. By exploring an
expressive posterior over the parameters of the generator, the Bayesian GAN
avoids mode-collapse, produces interpretable and diverse candidate samples, and
provides state-of-the-art quantitative results for semi-supervised learning on
benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,
Wasserstein GANs, and DCGAN ensembles.","[Submitted on 26 May 2017 (v1), last revised 8 Nov 2017 (this version, v3)]"
"1665689187-8","https://arxiv.org/abs/1706.05477","Title:Bayesian Conditional Generative Adverserial Networks","Authors:M. Ehsan Abbasnejad, Qinfeng Shi, Iman Abbasnejad, Anton van den Hengel, Anthony Dick","Abstract:  Traditional GANs use a deterministic generator function (typically a neural
network) to transform a random noise input z to a sample x that
the discriminator seeks to distinguish. We propose a new GAN called Bayesian
Conditional Generative Adversarial Networks (BC-GANs) that use a random
generator function to transform a deterministic input y′ to a sample
x. Our BC-GANs extend traditional GANs to a Bayesian framework, and
naturally handle unsupervised learning, supervised learning, and
semi-supervised learning problems. Experiments show that the proposed BC-GANs
outperforms the state-of-the-arts.","[Submitted on 17 Jun 2017]"
"1665689189-9","https://arxiv.org/abs/1707.05474","Title:APE-GAN: Adversarial Perturbation Elimination with GAN","Authors:Shiwei Shen, Guoqing Jin, Ke Gao, Yongdong Zhang","Abstract:  Although neural networks could achieve state-of-the-art performance while
recongnizing images, they often suffer a tremendous defeat from adversarial
examples--inputs generated by utilizing imperceptible but intentional
perturbation to clean samples from the datasets. How to defense against
adversarial examples is an important problem which is well worth researching.
So far, very few methods have provided a significant defense to adversarial
examples. In this paper, a novel idea is proposed and an effective framework
based Generative Adversarial Nets named APE-GAN is implemented to defense
against the adversarial examples. The experimental results on three benchmark
datasets including MNIST, CIFAR10 and ImageNet indicate that APE-GAN is
effective to resist adversarial examples generated from five attacks.","[Submitted on 18 Jul 2017 (v1), last revised 26 Sep 2017 (this version, v3)]"
"1665689191-10","https://arxiv.org/abs/1704.02304","Title:It Takes (Only) Two: Adversarial Generator-Encoder Networks","Authors:Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky","Abstract:  We present a new autoencoder-type architecture that is trainable in an
unsupervised mode, sustains both generation and inference, and has the quality
of conditional and unconditional samples boosted by adversarial learning.
Unlike previous hybrids of autoencoders and adversarial networks, the
adversarial game in our approach is set up directly between the encoder and the
generator, and no external mappings are trained in the process of learning. The
game objective compares the divergences of each of the real and the generated
data distributions with the prior distribution in the latent space. We show
that direct generator-vs-encoder game leads to a tight coupling of the two
components, resulting in samples and reconstructions of a comparable quality to
some recently-proposed more complex architectures.","[Submitted on 7 Apr 2017 (v1), last revised 6 Nov 2017 (this version, v3)]"
"1665689193-11","https://arxiv.org/abs/1702.05464","Title:Adversarial Discriminative Domain Adaptation","Authors:Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell","Abstract:  Adversarial learning methods are a promising approach to training robust deep
networks, and can generate complex samples across diverse domains. They also
can improve recognition despite the presence of domain shift or dataset bias:
several adversarial approaches to unsupervised domain adaptation have recently
been introduced, which reduce the difference between the training and test
domain distributions and thus improve generalization performance. Prior
generative approaches show compelling visualizations, but are not optimal on
discriminative tasks and can be limited to smaller shifts. Prior discriminative
approaches could handle larger domain shifts, but imposed tied weights on the
model and did not exploit a GAN-based loss. We first outline a novel
generalized framework for adversarial adaptation, which subsumes recent
state-of-the-art approaches as special cases, and we use this generalized view
to better relate the prior approaches. We propose a previously unexplored
instance of our general framework which combines discriminative modeling,
untied weight sharing, and a GAN loss, which we call Adversarial Discriminative
Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably
simpler than competing domain-adversarial methods, and demonstrate the promise
of our approach by exceeding state-of-the-art unsupervised adaptation results
on standard cross-domain digit classification tasks and a new more difficult
cross-modality object classification task.","[Submitted on 17 Feb 2017]"
"1665689195-12","https://arxiv.org/abs/1511.05644","Title:Adversarial Autoencoders","Authors:Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey","Abstract:  In this paper, we propose the ""adversarial autoencoder"" (AAE), which is a
probabilistic autoencoder that uses the recently proposed generative
adversarial networks (GAN) to perform variational inference by matching the
aggregated posterior of the hidden code vector of the autoencoder with an
arbitrary prior distribution. Matching the aggregated posterior to the prior
ensures that generating from any part of prior space results in meaningful
samples. As a result, the decoder of the adversarial autoencoder learns a deep
generative model that maps the imposed prior to the data distribution. We show
how the adversarial autoencoder can be used in applications such as
semi-supervised classification, disentangling style and content of images,
unsupervised clustering, dimensionality reduction and data visualization. We
performed experiments on MNIST, Street View House Numbers and Toronto Face
datasets and show that adversarial autoencoders achieve competitive results in
generative modeling and semi-supervised classification tasks.","[Submitted on 18 Nov 2015 (v1), last revised 25 May 2016 (this version, v2)]"
"1665689197-13","https://arxiv.org/abs/1701.02386","Title:AdaGAN: Boosting Generative Models","Authors:Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, Bernhard Schölkopf","Abstract:  Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an
effective method for training generative models of complex data such as natural
images. However, they are notoriously hard to train and can suffer from the
problem of missing modes where the model is not able to produce examples in
certain regions of the space. We propose an iterative procedure, called AdaGAN,
where at every step we add a new component into a mixture model by running a
GAN algorithm on a reweighted sample. This is inspired by boosting algorithms,
where many potentially weak individual predictors are greedily aggregated to
form a strong composite predictor. We prove that such an incremental procedure
leads to convergence to the true distribution in a finite number of steps if
each step is optimal, and convergence at an exponential rate otherwise. We also
illustrate experimentally that this procedure addresses the problem of missing
modes.","[Submitted on 9 Jan 2017 (v1), last revised 24 May 2017 (this version, v2)]"
"1665689200-14","https://arxiv.org/abs/1703.02000","Title:Activation Maximization Generative Adversarial Nets","Authors:Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Yong Yu, Jun Wang","Abstract:  Class labels have been empirically shown useful in improving the sample
quality of generative adversarial nets (GANs). In this paper, we mathematically
study the properties of the current variants of GANs that make use of class
label information. With class aware gradient and cross-entropy decomposition,
we reveal how class labels and associated losses influence GAN's training.
Based on that, we propose Activation Maximization Generative Adversarial
Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been
conducted to validate our analysis and evaluate the effectiveness of our
solution, where AM-GAN outperforms other strong baselines and achieves
state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we
demonstrate that, with the Inception ImageNet classifier, Inception Score
mainly tracks the diversity of the generator, and there is, however, no
reliable evidence that it can reflect the true sample quality. We thus propose
a new metric, called AM Score, to provide a more accurate estimation of the
sample quality. Our proposed model also outperforms the baseline methods in the
new metric.","[Submitted on 6 Mar 2017 (v1), last revised 16 Nov 2018 (this version, v9)]"
