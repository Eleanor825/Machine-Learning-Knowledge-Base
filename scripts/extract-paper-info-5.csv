web-scraper-order,web-scraper-start-url,paper-title,paper-authors,paper-abstract,paper-year
"1666640575-19","https://arxiv.org/abs/1611.04076","Title:Least Squares Generative Adversarial Networks","Authors:Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley","Abstract:  Unsupervised learning with generative adversarial networks (GANs) has proven
hugely successful. Regular GANs hypothesize the discriminator as a classifier
with the sigmoid cross entropy loss function. However, we found that this loss
function may lead to the vanishing gradients problem during the learning
process. To overcome such a problem, we propose in this paper the Least Squares
Generative Adversarial Networks (LSGANs) which adopt the least squares loss
function for the discriminator. We show that minimizing the objective function
of LSGAN yields minimizing the Pearson \chi^2 divergence. There are two
benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher
quality images than regular GANs. Second, LSGANs perform more stable during the
learning process. We evaluate LSGANs on five scene datasets and the
experimental results show that the images generated by LSGANs are of better
quality than the ones generated by regular GANs. We also conduct two comparison
experiments between LSGANs and regular GANs to illustrate the stability of
LSGANs.","[Submitted on 13 Nov 2016 (v1), last revised 5 Apr 2017 (this version, v3)]"
"1666640577-20","https://arxiv.org/abs/1605.05396","Title:Generative Adversarial Text to Image Synthesis","Authors:Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee","Abstract:  Automatic synthesis of realistic images from text would be interesting and
useful, but current AI systems are still far from this goal. However, in recent
years generic and powerful recurrent neural network architectures have been
developed to learn discriminative text feature representations. Meanwhile, deep
convolutional generative adversarial networks (GANs) have begun to generate
highly compelling images of specific categories, such as faces, album covers,
and room interiors. In this work, we develop a novel deep architecture and GAN
formulation to effectively bridge these advances in text and image model- ing,
translating visual concepts from characters to pixels. We demonstrate the
capability of our model to generate plausible images of birds and flowers from
detailed text descriptions.","[Submitted on 17 May 2016 (v1), last revised 5 Jun 2016 (this version, v2)]"
"1666640579-21","https://arxiv.org/abs/1703.05192","Title:Learning to Discover Cross-Domain Relations with Generative Adversarial Networks","Authors:Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, Jiwon Kim","Abstract:  While humans easily recognize relations between data from different domains
without any supervision, learning to automatically discover them is in general
very challenging and needs many ground-truth pairs that illustrate the
relations. To avoid costly pairing, we address the task of discovering
cross-domain relations given unpaired data. We propose a method based on
generative adversarial networks that learns to discover relations between
different domains (DiscoGAN). Using the discovered relations, our proposed
network successfully transfers style from one domain to another while
preserving key attributes such as orientation and face identity. Source code
for official implementation is publicly available
this https URL","[Submitted on 15 Mar 2017 (v1), last revised 15 May 2017 (this version, v2)]"
"1666640582-22","https://arxiv.org/abs/1411.1784","Title:Conditional Generative Adversarial Nets","Authors:Mehdi Mirza, Simon Osindero","Abstract:  Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.","[Submitted on 6 Nov 2014]"
"1666640584-23","https://arxiv.org/abs/1711.10337","Title:Are GANs Created Equal? A Large-Scale Study","Authors:Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet","Abstract:  Generative adversarial networks (GAN) are a powerful subclass of generative
models. Despite a very rich research activity leading to numerous interesting
GAN algorithms, it is still very hard to assess which algorithm(s) perform
better than others. We conduct a neutral, multi-faceted large-scale empirical
study on state-of-the art models and evaluation measures. We find that most
models can reach similar scores with enough hyperparameter optimization and
random restarts. This suggests that improvements can arise from a higher
computational budget and tuning more than fundamental algorithmic changes. To
overcome some limitations of the current metrics, we also propose several data
sets on which precision and recall can be computed. Our experimental results
suggest that future GAN research should be based on more systematic and
objective evaluation procedures. Finally, we did not find evidence that any of
the tested algorithms consistently outperforms the non-saturating GAN
introduced in \cite{goodfellow2014generative}.","[Submitted on 28 Nov 2017 (v1), last revised 29 Oct 2018 (this version, v4)]"
"1666640586-24","https://arxiv.org/abs/1508.06576","Title:A Neural Algorithm of Artistic Style","Authors:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge","Abstract:  In fine art, especially painting, humans have mastered the skill to create
unique visual experiences through composing a complex interplay between the
content and style of an image. Thus far the algorithmic basis of this process
is unknown and there exists no artificial system with similar capabilities.
However, in other key areas of visual perception such as object and face
recognition near-human performance was recently demonstrated by a class of
biologically inspired vision models called Deep Neural Networks. Here we
introduce an artificial system based on a Deep Neural Network that creates
artistic images of high perceptual quality. The system uses neural
representations to separate and recombine content and style of arbitrary
images, providing a neural algorithm for the creation of artistic images.
Moreover, in light of the striking similarities between performance-optimised
artificial neural networks and biological vision, our work offers a path
forward to an algorithmic understanding of how humans create and perceive
artistic imagery.","[Submitted on 26 Aug 2015 (v1), last revised 2 Sep 2015 (this version, v2)]"
"1666640588-25","https://arxiv.org/abs/1312.6114","Title:Auto-Encoding Variational Bayes","Authors:Diederik P Kingma, Max Welling","Abstract:  How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable
posterior distributions, and large datasets? We introduce a stochastic
variational inference and learning algorithm that scales to large datasets and,
under some mild differentiability conditions, even works in the intractable
case. Our contributions is two-fold. First, we show that a reparameterization
of the variational lower bound yields a lower bound estimator that can be
straightforwardly optimized using standard stochastic gradient methods. Second,
we show that for i.i.d. datasets with continuous latent variables per
datapoint, posterior inference can be made especially efficient by fitting an
approximate inference model (also called a recognition model) to the
intractable posterior using the proposed lower bound estimator. Theoretical
advantages are reflected in experimental results.","[Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)]"
