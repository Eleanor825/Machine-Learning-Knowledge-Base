REFERENCES
[1] J. L. Ba, J. R. Kiros, and G. E. Hinton (2016) Layer normalization. arXiv preprint arXiv:1607.06450. Cited by: §3.3.
[2] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le (2019) Attention augmented convolutional networks. In Proc. IEEE Int. Conf. Comp. Vis., Cited by: §2.3.
[3] Z. Cai and N. Vasconcelos (2018) Cascade r-cnn: delving into high quality object detection. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.2.
[4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko (2020) End-to-end object detection with transformers. In Proc. Eur. Conf. Comp. Vis., Cited by: Figure 1, §1, §1, §2.2, §2.3, §5.4, §5.4, Table 6.
[5] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu, et al. (2019) MMDetection: open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155. Cited by: §5.2.
[6] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille (2017) Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell.. Cited by: §2.2, §5.3.
[7] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam (2018) Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proc. Eur. Conf. Comp. Vis., Cited by: §1.
[8] Y. Chen, J. Li, H. Xiao, X. Jin, S. Yan, and J. Feng (2017) Dual path networks. Proc. Advances in Neural Inf. Process. Syst.. Cited by: §2.1.
[9] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §1, §5.3, §5.5.
[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. (2021) An image is worth 16x16 words: transformers for image recognition at scale. Proc. Int. Conf. Learn. Representations. Cited by: 0(b), Figure 1, Figure 2, §1, §2.3, §3.5, §4.1, Figure 6, §5.1, §5.5, Table 2, Table 8.
[11] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman (2010) The pascal visual object classes (voc) challenge. Int. J. Comput. Vision 88 (2), pp. 303–338. Cited by: §1.
[12] S. Gao, M. Cheng, K. Zhao, X. Zhang, M. Yang, and P. H. Torr (2019) Res2net: a new multi-scale backbone architecture. IEEE Trans. Pattern Anal. Mach. Intell.. Cited by: §6.
[13] X. Glorot and Y. Bengio (2010) Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, Cited by: §5.2, §5.3.
[14] K. He, G. Gkioxari, P. Dollár, and R. Girshick (2017) Mask r-cnn. In Proc. IEEE Int. Conf. Comp. Vis., Cited by: §1, §2.2, §4.2, §5.2, Table 3, Table 4.
[15] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: 0(a), Figure 2, 3rd item, §1, §1, §2.1, §3.1, §3.2, §3.4, Table 1, Figure 5, Figure 6, §5.1, §5.2, §5.3, §5.5, §5.5, Table 10, Table 2, Table 3, Table 4, Table 5, Table 6, §5.
[16] J. Hu, L. Shen, and G. Sun (2018) Squeeze-and-excitation networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pp. 7132–7141. Cited by: §6.
[17] R. Hu and A. Singh (2211) Transformer is all you need: multimodal multitask learning with a unified transformer. arXiv preprint arXiv:2102.10772. Cited by: §1.
[18] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger (2017) Densely connected convolutional networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.1.
[19] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu (2019) Ccnet: criss-cross attention for semantic segmentation. In Proc. IEEE Int. Conf. Comp. Vis., Cited by: §2.3.
[20] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi (2020) A survey of the recent architectures of deep convolutional neural networks. Artificial Intelligence Review 53 (8), pp. 5455–5516. Cited by: §2.1.
[21] A. Kirillov, R. Girshick, K. He, and P. Dollár (2019) Panoptic feature pyramid networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pp. 6399–6408. Cited by: §1, §2.2, §4.2, §5.3.
[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton (2012) Imagenet classification with deep convolutional neural networks. Proc. Advances in Neural Inf. Process. Syst.. Cited by: §2.1.
[23] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner (1998) Gradient-based learning applied to document recognition. Cited by: §2.1, §2.2.
[24] X. Li, W. Wang, X. Hu, and J. Yang (2019) Selective kernel networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.1, §6.
[25] X. Li, W. Wang, L. Wu, S. Chen, X. Hu, J. Li, J. Tang, and J. Yang (2020) Generalized focal loss: learning qualified and distributed bounding boxes for dense object detection. In Proc. Advances in Neural Inf. Process. Syst., Cited by: §2.2.
[26] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie (2017) Feature pyramid networks for object detection. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.2, §4.2.
[27] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár (2017) Focal loss for dense object detection. In Proc. IEEE Int. Conf. Comp. Vis., Cited by: §1, §1, §2.2, §4.2, §5.2, §5.5, Table 3, Table 4.
[28] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick (2014) Microsoft coco: common objects in context. In Proc. Eur. Conf. Comp. Vis., Cited by: §1, §5.2, §5.5.
[29] C. Liu, L. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and L. Fei-Fei (2019) Auto-deeplab: hierarchical neural architecture search for semantic image segmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.2.
[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and A. C. Berg (2016) Ssd: single shot multibox detector. In Proc. Eur. Conf. Comp. Vis., Cited by: §2.2.
[31] J. Long, E. Shelhamer, and T. Darrell (2015) Fully convolutional networks for semantic segmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.2.
[32] I. Loshchilov and F. Hutter (2016) Sgdr: stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983. Cited by: §5.1.
[33] I. Loshchilov and F. Hutter (2017) Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Cited by: §5.1, §5.2, §5.3.
[34] H. Noh, S. Hong, and B. Han (2015) Learning deconvolution network for semantic segmentation. In Proc. IEEE Int. Conf. Comp. Vis., Cited by: §2.2.
[35] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens (2019) Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909. Cited by: §1, §2.3.
[36] S. Ren, K. He, R. Girshick, and J. Sun (2015) Faster r-cnn: towards real-time object detection with region proposal networks. In Proc. Advances in Neural Inf. Process. Syst., Cited by: §1, §2.2.
[37] O. Ronneberger, P. Fischer, and T. Brox (2015) U-net: convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, Cited by: §2.2.
[38] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. (2015) Imagenet large scale visual recognition challenge. Int. J. Comput. Vision. Cited by: §2.1, §5.1.
[39] S. Shetty (2016) Application of convolutional neural network for image classification on pascal voc challenge 2012 dataset. arXiv preprint arXiv:1607.03785. Cited by: §2.2.
[40] C. Shorten and T. M. Khoshgoftaar (2019) A survey on image data augmentation for deep learning. Journal of Big Data 6 (1), pp. 1–48. Cited by: §2.1.
[41] K. Simonyan and A. Zisserman (2014) Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Cited by: 0(a), §1, §1, §2.1.
[42] P. Sun, Y. Jiang, E. Xie, Z. Yuan, C. Wang, and P. Luo (2020) OneNet: towards end-to-end one-stage object detection. arXiv preprint arXiv:2012.05780. Cited by: §2.2.
[43] P. Sun, Y. Jiang, R. Zhang, E. Xie, J. Cao, X. Hu, T. Kong, Z. Yuan, C. Wang, and P. Luo (2020) TransTrack: multiple-object tracking with transformer. arXiv preprint arXiv:2012.15460. Cited by: §1.
[44] P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka, L. Li, Z. Yuan, C. Wang, et al. (2020) Sparse r-cnn: end-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450. Cited by: §2.2.
[45] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi (2017) Inception-v4, inception-resnet and the impact of residual connections on learning. In Proc. AAAI Conf. Artificial Intell., Cited by: §2.1.
[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich (2015) Going deeper with convolutions. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.1, §5.1.
[47] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna (2016) Rethinking the inception architecture for computer vision. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.1, §5.1.
[48] M. Tan and Q. Le (2019) Efficientnet: rethinking model scaling for convolutional neural networks. In Proc. Int. Conf. Mach. Learn., Cited by: §6.
[49] Z. Tian, C. Shen, H. Chen, and T. He (2019) Fcos: fully convolutional one-stage object detection. In Proc. IEEE Int. Conf. Comp. Vis., Cited by: §2.2.
[50] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou (2020) Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877. Cited by: §2.3, §4.1, §5.1, §5.1, Table 2.
[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2017) Attention is all you need. arXiv preprint arXiv:1706.03762. Cited by: §1, §2.3, 6th item, §3.3, §3.3, §3.5.
[52] W. Wang, X. Li, J. Yang, and T. Lu (2018) Mixed link networks. Proc. Int. Joint Conf. Artificial Intell.. Cited by: §2.1.
[53] X. Wang, R. Girshick, A. Gupta, and K. He (2018) Non-local neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §1, §2.3.
[54] E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, and P. Luo (2020) Polarmask: single shot instance segmentation with polar representation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pp. 12193–12202. Cited by: §2.2.
[55] E. Xie, W. Wang, W. Wang, P. Sun, H. Xu, D. Liang, and P. Luo (2021) Segmenting transparent object in the wild with transformer. arXiv preprint arXiv:2101.08461. Cited by: §1, §5.4, §5.4, Table 7.
[56] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He (2017) Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: Figure 2, 3rd item, §1, §2.1, §5.2, §5.3, Table 2, Table 3, Table 4, Table 5, §5.
[57] F. Yu and V. Koltun (2015) Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122. Cited by: §5.3, §6.
[58] E. Zerhouni, D. Lányi, M. Viana, and M. Gabrani (2017) Wide residual networks for mitosis detection. In 2017 IEEE 14th International Symposium on Biomedical Imaging, pp. 924–928. Cited by: §5.5.
[59] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, Z. Zhang, H. Lin, Y. Sun, T. He, J. Mueller, R. Manmatha, et al. (2020) Resnest: split-attention networks. arXiv preprint arXiv:2004.08955. Cited by: §6.
[60] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz (2017) Mixup: beyond empirical risk minimization. arXiv preprint arXiv:1710.09412. Cited by: §5.1.
[61] H. Zhao, J. Jia, and V. Koltun (2020) Exploring self-attention for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pp. 10076–10085. Cited by: §1.
[62] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia (2017) Pyramid scene parsing network. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §2.2.
[63] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba (2017) Scene parsing through ade20k dataset. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., Cited by: §1, §5.3, §5.4.
[64] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai (2020) Deformable detr: deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159. Cited by: §1, §2.2, §2.3.
