References
[1] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas, “Predicting Parameters in Deep Learning,” in Neural Information Processing Systems (NIPS), 2013, pp. 2148–2156. arXiv:1306 . 0543 (cit. on p. 1).
[2] C Szegedy, W Liu, Y Jia, and P Sermanet, “Going deeper with convolutions,” in arXiv preprint arXiv: 1409.4842, 2014, ISBN: 9781467369640. arXiv:1602.07360 (cit. on pp. 1, 2, 6).
[3] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, “Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation,” in arXiv, 2014, pp. 1–11. arXiv:1404.0736 (cit. on p. 1).
[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks,” in Advances In Neural Information Processing Systems, P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds., 2012, pp. 1–9, ISBN: 9781627480031. arXiv:1102. 0183 (cit. on pp. 1, 2).
[5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, Improving neural networks by preventing co-adaptation of feature detectors, 2012. arXiv:1207.0580 (cit. on pp. 1, 3).
[6] K Fukushima, “Neocognitron: A self-organizing neural network model for a mechanish of pattern recognition unaffected by shifts in position,” Biological Cybernetics, vol. 36, pp. 193–202, 1980 (cit. on p. 1).
[7] Y Lecun, L Bottou, Y Bengio, and P Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998, ISSN: 0018-9219 (cit. on p. 1).
[8] M. Minsky and S. Papert, Perceptrons. MIT press, 1988 (cit. on p. 1).
[9] Y. Ioannou, D. P. Robertson, J. Shotton, R. Cipolla, and A. Criminisi, “Training CNNs with Low-Rank Filters for Efficient Image Classification,” in International Conference on Learning Representations, 2016 (cit. on pp. 1, 3, 5, 6, 8).
[10] F. Mamalet and C. Garcia, “Simplifying convnets for fast learning,” in Artificial Neural Networks and Machine Learning–ICANN 2012, Springer, 2012, pp. 58–65 (cit. on p. 1).
[11] M. Jaderberg, A. Vedaldi, and A. Zisserman, “Speeding up Convolutional Neural Networks with Low Rank Expansions.,” in British Machine Vision Conference, 2014 (cit. on pp. 1, 2).
[12] A. Sironi, B. Tekin, R. Rigamonti, V. Lepetit, and P. Fua, “Learning separable filters,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 1, pp. 94–106, 2015, ISSN: 01628828 (cit. on p. 1).
[13] V. Lebedev, Y. Ganin, M. Rakhuba1, I. Oseledets, and V. Lempitsky, “Speeding-Up Convolutional Neural Networks Using Fine-tuned CP-Decomposition,” International Conference on Learning Representations (ICLR), vol. abs/1412.6, pp. 1–10, 2015. arXiv:arXiv : 1412 . 6553v2 (cit. on pp. 1, 2).
[14] M. Mathieu, M. Henaff, and Y LeCun, “Fast Training of Convolutional Networks through FFTs,” International Conference on Learning Representations (ICLR), pp. 1–9, 2014. arXiv:arXiv : 1312 . 5851v5 (cit. on p. 1).
[15] O. Rippel, J. Snoek, and R. P. Adams, “Spectral Representations for Convolutional Neural Networks,” Advances in Neural Information Processing Systems 28, pp. 2440–2448, 2015, ISSN: 10495258. arXiv:1506.03767 (cit. on p. 1).
[16] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, Deep Learning with Limited Numerical Precision, 2015. arXiv:1502.02551 (cit. on p. 1).
[17] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen, “Compressing Neural Networks with the Hashing Trick,” in Proceedings of The 32nd International Conference on Machine Learning, F. R. Bach and D. M. Blei, Eds., ser. JMLR Proceedings, vol. 37, JMLR.org, 2015, pp. 2285–2294, ISBN: 9781510810587. arXiv:1504.04788 (cit. on p. 1).
[18] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin, “Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications,” in International Conference on Learning Representations (ICLR), 2016, pp. 1–16. arXiv:1511. 06530 (cit. on pp. 1, 2).
[19] M. Lin, Q. Chen, and S. Yan, “Network In Network,” arXiv preprint, vol. abs/1312.4, p. 10, 2013. arXiv:1312.4400 (cit. on pp. 2, 4, 5).
[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” Arxiv.Org, vol. 7, no. 3, pp. 171–180, 2015, ISSN: 1664-1078. arXiv:1512.03385 (cit. on pp. 2, 4, 5).
[21] M. Cogswell, F. Ahmed, R. B. Girshick, L. Zitnick, and D. Batra, “Reducing Overfitting in Deep Networks by Decorrelating Representations.,” in International Conference on Learning Representations, 2016 (cit. on p. 3).
[22] A. Krizhevsky, “Learning Multiple Layers of Features from Tiny Images,” Univ. Toronto, Technical Report, 2009, pp. 1–60. arXiv:arXiv:1011. 1669v3 (cit. on pp. 4, 11).
[23] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, “Maxout Networks,” in Proceedings of the 30th International Conference on Machine Learning (ICML), vol. 28, 2013, pp. 1319– 1327. arXiv:1302.4389 (cit. on p. 4).
[24] K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” in IEEE Conference on Computer Vision and Patern Recognition (ICCV), IEEE, 2015, pp. 1026–1034, ISBN: 978-1- 4673-8391-2. arXiv:1502.01852 (cit. on pp. 4–6).
[25] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.,” in Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015, 2015 (cit. on pp. 4–6).
[26] K Simonyan and A Zisserman, “Very deep convolutional networks for large-scale image recognition,” in eprint ar{X}iv:arXiv:1409.1556v5, 1409 (cit. on p. 5).
[27] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional Architecture for Fast Feature Embedding,” ACM International Conference on Multimedia, pp. 675–678, 2014, ISSN: 10636919. arXiv:1408.5093 (cit. on p. 6).
[28] C. Jhurani and P. Mullowney, “A GEMM interface and implementation on NVIDIA GPUs for multiple small matrices,” Journal of Parallel and Distributed Computing, vol. 75, pp. 133–140, 2015 (cit. on p. 8).
