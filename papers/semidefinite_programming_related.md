<span style="font-family:monospace">

# Papers Related to Semidefinite Programming

* [Semidefinite relaxations for certifying robustness to adversarial examples](https://arxiv.org/abs/1811.01057)
    * Year: 02 Nov 2018
    * Author: Aditi Raghunathan
    * Abstract: Despite their impressive performance on diverse tasks, neural networks fail catastrophically in the presence of adversarial inputs---imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different "foreign networks" whose training objectives are agnostic to our proposed relaxation.

* [A Survey of Recent Scalability Improvements for Semidefinite Programming with Applications in Machine Learning, Control, and Robotics](https://arxiv.org/abs/1908.05209)
    * Year: 14 Aug 2019
    * Author: Anirudha Majumdar
    * Abstract: Historically, scalability has been a major challenge to the successful application of semidefinite programming in fields such as machine learning, control, and robotics. In this paper, we survey recent approaches for addressing this challenge including (i) approaches for exploiting structure (e.g., sparsity and symmetry) in a problem, (ii) approaches that produce low-rank approximate solutions to semidefinite programs, (iii) more scalable algorithms that rely on augmented Lagrangian techniques and the alternating direction method of multipliers, and (iv) approaches that trade off scalability with conservatism (e.g., by approximating semidefinite programs with linear and second-order cone programs). For each class of approaches we provide a high-level exposition, an entry-point to the corresponding literature, and examples drawn from machine learning, control, or robotics. We also present a list of software packages that implement many of the techniques discussed in the paper. Our hope is that this paper will serve as a gateway to the rich and exciting literature on scalable semidefinite programming for both theorists and practitioners.

* [Learning with Semi-Definite Programming: new statistical bounds based on fixed point analysis and excess risk curvature](https://arxiv.org/abs/2004.01869)
    * Year: 04 Apr 2020
    * Author: Stéphane Chrétien
    * Abstract: Many statistical learning problems have recently been shown to be amenable to Semi-Definite Programming (SDP), with community detection and clustering in Gaussian mixture models as the most striking instances [javanmard et al., 2016]. Given the growing range of applications of SDP-based techniques to machine learning problems, and the rapid progress in the design of efficient algorithms for solving SDPs, an intriguing question is to understand how the recent advances from empirical process theory can be put to work in order to provide a precise statistical analysis of SDP estimators. In the present paper, we borrow cutting edge techniques and concepts from the learning theory literature, such as fixed point equations and excess risk curvature arguments, which yield general estimation and prediction results for a wide class of SDP estimators. From this perspective, we revisit some classical results in community detection from [guédon et al.,2016] and [chen et al., 2016], and we obtain statistical guarantees for SDP estimators used in signed clustering, group synchronization and MAXCUT.

* [Partition-Based Convex Relaxations for Certifying the Robustness of ReLU Neural Networks](https://arxiv.org/abs/2101.09306)
    * Year: 22 Jan 2021
    * Author: Brendon G. Anderson
    * Abstract: In this paper, we study certifying the robustness of ReLU neural networks against adversarial input perturbations. To diminish the relaxation error suffered by the popular linear programming (LP) and semidefinite programming (SDP) certification methods, we propose partitioning the input uncertainty set and solving the relaxations on each part separately. We show that this approach reduces relaxation error, and that the error is eliminated entirely upon performing an LP relaxation with an intelligently designed partition. To scale this approach to large networks, we consider courser partitions that take the same form as this motivating partition. We prove that computing such a partition that directly minimizes the LP relaxation error is NP-hard. By instead minimizing the worst-case LP relaxation error, we develop a computationally tractable scheme with a closed-form optimal two-part partition. We extend the analysis to the SDP, where the feasible set geometry is exploited to design a two-part partition that minimizes the worst-case SDP relaxation error. Experiments on IRIS classifiers demonstrate significant reduction in relaxation error, offering certificates that are otherwise void without partitioning. By independently increasing the input size and the number of layers, we empirically illustrate under which regimes the partitioned LP and SDP are best applied.

* [Statistical Inference of Semidefinite Programming](http://www.optimization-online.org/DB_FILE/2017/01/5842.pdf)
    * Author: Alexander Shapiro
    * Abstract: In this paper we consider covariance structural models with which we associate semidefinite programming problems. We discuss statistical properties of estimates of the respective optimal value and optimal solutions when the ‘true’ covariance matrix is estimated by its sample counterpart. The analysis is based on perturbation theory of semidefinite programming. As an example we consider asymptotics of the so-called Minimum Trace Factor Analysis. We also discuss the Minimum Rank Matrix Completion problem and its SDP counterparts.

* [Strengthened SDP Verification of Neural Network Robustness via Non-Convex Cuts](https://people.eecs.berkeley.edu/~sojoudi/Ma_Sojoudi.pdf)
    * Author: Ziye Ma
    * Abstract: There have been major advances on the design of neural networks, but still they cannot be applied to many safety-critical systems due to the lack of efficient computational techniques to analyze and certify their robustness. Recently, various methods based on convex optimization have been proposed to address this issue. In particular, the semidefinite programming (SDP) approach has gained popularity in convexifying the robustness analysis problem. Since this approach is prone to a large relaxation gap, this paper develops a new technique to reduce the gap by adding non-convex cuts via disjunctive programming. The proposed method amounts to a sequential SDP technique. We analyze the performance of this method both theoretically and empirically, and show that it bridges the gap as the number of cuts increases.
