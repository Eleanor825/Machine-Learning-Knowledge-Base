<span style="font-family:monospace">

# Papers in Computer Vision - CNN Architectures

count: 14

* [AlexNet](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)
    * Title: `ImageNet Classification with Deep Convolutional Neural Networks`
    * Year: 2012
    * Author: Alex Krizhevsky
* [DropConnect](https://proceedings.mlr.press/v28/wan13.html)
    * Title: `Regularization of Neural Networks using DropConnect`
    * Year: 2013
    * Author: Li Wan
* [Maxout](https://arxiv.org/abs/1302.4389)
    * Title: `Maxout Networks`
    * Year: 18 Feb 2013
    * Author: Ian J. Goodfellow
* [Network In Network](https://arxiv.org/abs/1312.4400)
    * Title: `Network In Network`
    * Year: 16 Dec 2013
    * Author: Min Lin
* [DSN](https://arxiv.org/abs/1409.5185)
    * Title: `Deeply-Supervised Nets`
    * Year: 18 Sep 2014
    * Author: Chen-Yu Lee
* [Highway Networks](https://arxiv.org/abs/1507.06228)
    * Title: `Training Very Deep Networks`
    * Year: 22 Jul 2015
    * Author: Rupesh Kumar Srivastava
* [FractalNet](https://arxiv.org/abs/1605.07648)
    * Title: `FractalNet: Ultra-Deep Neural Networks without Residuals`
    * Year: 24 May 2016
    * Author: Gustav Larsson
* [DFN](https://arxiv.org/abs/1605.07716)
    * Title: `Deeply-Fused Nets`
    * Year: 25 May 2016
    * Author: Jingdong Wang
* [DenseNet](https://arxiv.org/abs/1608.06993)
    * Title: `Densely Connected Convolutional Networks`
    * Year: 25 Aug 2016
    * Author: Gao Huang
* [ConvNeXt](https://arxiv.org/abs/2201.03545)
    * Title: `A ConvNet for the 2020s`
    * Year: 10 Jan 2022
    * Author: Zhuang Liu

## Inception Network Series

* [InceptionV1/GoogLeNet](https://arxiv.org/abs/1409.4842)
    * Title: `Going Deeper with Convolutions`
    * Year: 17 Sep 2014
    * Author: Christian Szegedy
    * Abstract: We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.
* [InceptionV2](https://arxiv.org/abs/1512.00567)
    * Title: `Rethinking the Inception Architecture for Computer Vision`
    * Year: 02 Dec 2015
    * Author: Christian Szegedy

## ResNet and its Variants

* [ResNet](https://arxiv.org/abs/1512.03385)
    * Title: `Deep Residual Learning for Image Recognition`
    * Year: 10 Dec 2015
    * Author: Kaiming He
* [ResNet with Pre-Activation](https://arxiv.org/abs/1603.05027)
    * Title: `Identity Mappings in Deep Residual Networks`
    * Year: 16 Mar 2016
    * Author: Kaiming He
* [Generalized ResNet](https://arxiv.org/abs/1603.08029)
    * Title: `Resnet in Resnet: Generalizing Residual Architectures`
    * Year: 25 Mar 2016
    * Author: Sasha Targ
* [Stochastic Depth](https://arxiv.org/abs/1603.09382)
    * Title: `Deep Networks with Stochastic Depth`
    * Year: 30 Mar 2016
    * Author: Gao Huang
* [Wide ResNet](https://arxiv.org/abs/1605.07146)
    * Title: `Wide Residual Networks`
    * Year: 23 May 2016
    * Author: Sergey Zagoruyko
