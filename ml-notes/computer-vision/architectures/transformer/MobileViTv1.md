# [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178)

* Year: 05 Oct `2021`
* Author: Sachin Mehta
* Abstract: Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: [this https URL](https://github.com/apple/ml-cvnets).

----------------------------------------------------------------------------------------------------

## 1 INTRODUCTION

> Note that floating-point operations (FLOPs) are not sufficient for low latency on mobile devices because FLOPs ignore several important inference-related factors such as memory access, degree of parallelism, and platform characteristics.

> Therefore, instead of optimizing for FLOPs, this paper focuses on designing a light-weight, general-purpose, and low latency network for mobile vision tasks. We achieve this goal with MobileViT that combines the benefits of CNNs (e.g., spatial inductive biases and less sensitivity to data augmentation) and ViTs (e.g., input-adaptive weighting and global processing). Specifically, we introduce the MobileViT block that encodes both local and global information in a tensor effectively. Unlike ViT and its variants (with and without convolutions), MobileViT presents a different perspective to learn global representations. Standard convolution involves three operations: unfolding, local processing, and folding. MobileViT block replaces local processing in convolutions with global processing using transformers. This allows MobileViT block to have CNN- and ViT-like properties, which helps it learn better representations with fewer parameters and simple training recipes (e.g., basic augmentation).

## 2 RELATED WORK

## 3 MOBILEVIT: A LIGHT-WEIGHT TRANSFORMER

> This paper introduces a light-weight ViT model, MobileViT. The core idea is to learn global representations with transformers as convolutions. This allows us to implicitly incorporate convolution-like properties (e.g., spatial bias) in the network, learn representations with simple training recipes (e.g., basic augmentation), and easily integrate MobileViT with downstream architectures (e.e., DeepLabv3 for segmentation).

### 3.1 MOBILEVIT ARCHITECTURE

> **MobileViT block.** The MobileViT block aims to model the local and global information in an input tensor with fewer parameters. Formally, for a given input tensor $\textbf{X} \in \mathbb{R}^{H \times W \times C}$, MobileViT applies a $n \times n$ standard convolutional layer followed by a point-wise (or $1 \times 1$) convolutional layer to produce $\textbf{X}_{L} \in \mathbb{R}^{H \times W \times d}$. The $n \times n$ convolutional layer encodes local spatial information while the point-wise convolution projects the tensor to a high-dimensional space (or $d$-dimensional, where $d > C$) by learning linear combinations of the input channels.

> With MobileViT, we want to model long-range non-local dependencies while having an effective receptive field of $H \times W$. One of the widely studied methods to model long-range dependencies is dilated convolutions. However, such approaches require careful selection of dilation rates. Otherwise, weights are applied to padded zeros in stead of the valid spatial region. Another promising solution is self-attention. Among self--attention methods,vision transformers (ViTs) with multi-head self-attention are shown to be effective for visual recognition tasks. However, ViTs are heavy-weight and exhibit sub-standard optimizability. This is because ViTs lack spatial inductive bias.

> To enable MobileViT to learn global representations with spatial inductive bias, we unfold $\textbf{X}_{L}$ into $N$ non-overlapping flattened patches $\textbf{X}_{U} \in \mathbb{R}^{P \times N \times d}$. Here, $P = wh$, $N = \frac{HW}{P}$ is the number of patches, and $h \leq n$ and $w \leq n$ are height and width of a patch respectively. For each $p \in \{1, ..., P\}$, inter-patch relationships are encoded by applying transformers to obtain $\textbf{X}_{G} \in \mathbb{R}^{P \times N \times d}$ as:
> $$\textbf{X}_{G}(p) = \text{Transformer}(\textbf{X}_{U}(p)), 1 \leq p \leq P, \tag{1}$$
> Unlike ViTs that lose the spatial order of pixels, Mobile ViT neither loses the patch order nor the spatial order of pixels within each path. Therefore, we can fold $\textbf{X}_{G} \in \mathbb{R}^{P \times N \times d}$ to obtain $\textbf{X}_{F} \in \mathbb{R}^{H \times W \times d}$. $\textbf{X}_{F}$ is then projected to low $C$-dimensional space using a point-wise convolution and combined with $\textbf{X}$ via concatenation operation. Another $n \times n$ convolutional layer is then used to fuse these concatenated features. Note that because $\textbf{X}_{U}(p)$ encodes local information from $n \times n$ region using convolutions and $\textbf{X}_{G}(p)$ encodes global information across $P$ patches for the $p$-th location, each pixel in $\textbf{X}_{G}$ can encode information from all pixels in $\textbf{X}$. Thus, the overall effective receptive field of MobileViT is $H \times W$.

* $\textbf{X} \in \mathbb{R}^{H \times W \times C}$ input.
* $\textbf{X}_{L} \in \mathbb{R}^{H \times W \times d}$ by conv + point-wise on $\textbf{X}$.
* $\textbf{X}_{U} \in \mathbb{R}^{P \times N \times d}$ by unfolding $\textbf{X}_{L}$.
* $\textbf{X}_{G} \in \mathbb{R}^{P \times N \times d}$ by Transformer on $\textbf{X}_{U}$.
* $\textbf{X}_{F} \in \mathbb{R}^{H \times W \times d}$ by folding $\textbf{X}_{G}$.
* $Y$ by point-wise + conv on $\textbf{X}_{F}$.

> **Relationship to convolutions.** Standard convolutions can be viewed as a stack of three sequential operations: (1) unfolding, (2) matrix multiplication (to learn local representations), and (3) folding. Mobile ViT block is similar to convolutions in the sense that it also leverages the same building blocks. MobileViT block replaces the local processing (matrix multiplication) in convolutions with deeper global processing (a stack of transformer layers). As a consequence, MobileViT has convolution-like properties (e.g., spatial bias). Hence, the MobileViT block can be viewed as transformers as convolutions. An advantage of our intentionally simple design is that low-level efficient implementations of convolutions and transformers can be used out-of-the box; allowing us to use MobileViT on different devices without any extra effort.

> **Light-weight.** MobileViT block uses standard convolutions and transformers to learn local and global representations respectively. Because previous works have shown that networks designed using these layers are heavy-weight, a natural question arises: Why MobileViT is light-weight? We believe that the issues lie primarily in learning global representations with transformers. For a given patch, previous works convert the spatial information into latent by learning a linear combination of pixels. The global information is then encoded by learning inter-patch information using transformers. As a result, these models lose image-specific inductive ias, which is inherent in CNNs. Therefore, they require more capacity to learn visual representations. Hence, they are deep and wide. Unlike these models, MobileViT uses convolutions and transformers in a way that the resultant MobileViT block has convolution-like properties while simultaneously allowing for global processing. This modeling capability allows us to design shallow and narrow MobileViT models, which in turn are light-weight. Compared to the ViT-based model DeIT that uses $L = 12$ and $d = 192$, MobileViT model uses $L = \{2, 4, 3\}$ and $d = \{96, 120, 144\}$ at spatial levels $32 \times 32$, $16 \times 16, and $8 \times 8$, respectively.

> **Computational cost.**

> **MobileViT architecture.** Our networks are inspired by the philosophy of light-weight CNNs.
> The initial layer in MobileViT is a strided $3 \times 3$ standard convolution, followed by MobileNetv2 (or MV2) blocks and MobileViT blocks.
> We use Swish as an activation function.

### 3.2 MULTI-SCALE SAMPLER FOR TRAINING EFFICIENCY

> To facilitate MobileViT learn multi-scale representations without fine-tuning and to further improve training efficiency (i.e., fewer optimization updates), we extend the multi-scale training method to variably-sized batch sizes. Given a sorted set of spatial resolutions $\mathcal{S} = \{(H_{1},W_{1}), ..., (H_{n}, W_{n})\}$ and a batch size $b$ for a maximum spatial resolution of $(H_{n}, W_{n})$, we randomly sample spatial resolution $(H_{t}, W_{t}) \in \mathcal{S}$ at $t$-th training iteration on each GPU and compute the batch size for $t$-th iteration as $b_{t} = \frac{H_{n}W_{n}}{H_{t}W_{t}}b$, As a result, larger batch sizes are used for smaller spatial resolutions. This reduces optimizer updates per epoch and helps in faster training.
